{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2abffc1f-e43b-4c8c-884a-0a7b6cf9f2d0",
   "metadata": {},
   "source": [
    "# Bias Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fd792a-bb0d-4879-ab66-b552ee3d788c",
   "metadata": {},
   "source": [
    "Bias = a prejudice or inclination for or against something, often in an unfair way, or as a systematic distortion of results or perceptions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30606afb-3ed3-4f08-b346-6f1b6b84aaa5",
   "metadata": {},
   "source": [
    "#### Advanced Method Institution Bias Detection\n",
    "\n",
    "1)  Source & provenance analysis (who and how)  \n",
    "- What they do: check the outlet’s ownership, funders, editorial line, staff expertise, and historical bias; verify authorship and publication metadata; track a story’s chain of custody.\n",
    "- Why: bias often flows from ownership incentives, sponsorship and editorial policy. Fact-checkers and media monitors systematically record source metadata and compare across outlets\n",
    ">outlet ownership / funding disclosures  \n",
    "author history and beat expertise  \n",
    "use of unnamed/anonymous sources (frequency & pattern)  \n",
    "links / citations used (source diversity)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564b3dc8-91aa-4143-a495-148636acce2a",
   "metadata": {},
   "source": [
    "2) Claim Extraction → Fact-Check Pipeline\n",
    "- Goal: Extract verifiable factual claims and auto-match them to fact-check databases.\n",
    "- Libraries: spaCy or Stanza (for NER, dependency parsing), transformers (for claim detection), elasticsearch.\n",
    "> NER + dependency parsing to extract claims → normalize claims → search fact-check KB (fuzzy match) → present candidate matches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b576eda1-aa36-41ea-8aaf-60ee89aefc02",
   "metadata": {},
   "source": [
    "3) Cross-Outlet Framing Comparator (event-level)\n",
    "- Goal: For a single event, compare topic distribution, named actors mentioned, sentiment by actor, and omitted facts across outlets.\n",
    "- Libraries: transformers (BERT / XLM-RoBERTa), NLTK, gensim (topic modeling), networkx, matplotlib/plotly.\n",
    "> align articles by event (using dates + keywords) → extract topics & entities → compute per-outlet distributions and divergence metrics (KL divergence) → visualize."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cc9f53-6a49-4b13-bee8-7fda1766dfec",
   "metadata": {},
   "source": [
    "4) Stance Detection Classifier\n",
    "- Goal: Train a model to label sentences as pro/anti/neutral toward a target (policy/person).\n",
    "- Libraries: transformers (fine-tune RoBERTa/BERT), sklearn, datasets.\n",
    "> fine-tune transformer for stance → evaluate cross-target generalization → test on news sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d6c854-9db3-4885-a19c-cddd856fd38d",
   "metadata": {},
   "source": [
    "5) Lexical Bias Detector (hedging & loaded language)\n",
    "\n",
    "- Goal: Count and score rhetorical devices (hedges, intensifiers, opinion verbs, passive voice) indicating slant.\n",
    "- Libraries: spaCy (tagging, dependency), textstat, lexicon libs.\n",
    "> compute per-article feature vector (hedges per 1k tokens, passive ratio, emotive adj), train a classifier to predict left/right/centrist labels or compare distributions across outlets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5db97b-a1fa-4a22-976f-ea75e3934198",
   "metadata": {},
   "source": [
    "6) Source-Network / Amplification Graph\n",
    "\n",
    "- Goal: Build and visualize the graph of which sources an article cites and which social accounts amplify it. Detect echo chambers.\n",
    "- Libraries: NetworkX, SNAP/igraph, Tweepy (Twitter API) or CrowdTangle for FB/IG (if access), BeautifulSoup.\n",
    "> extract outgoing links and social shares → build graph → compute modularity, centrality, clustering → detect insular clusters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd1611d-0630-4e23-b9bd-784bcfd1d1a6",
   "metadata": {},
   "source": [
    "7) Multimodal Forensics (image/video)\n",
    "\n",
    "- Goal: Run quick forensic checks on images embedded in news (reverse image, ELA).\n",
    "- Libraries: OpenCV, pillow, imagehash, requests, Tineye / Google Reverse Image Search (scripted), ffmpeg for video frames.\n",
    ">extract images → check EXIF → compute image hash and search reverse image → run ELA for manipulation → flag suspicious items.\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b315f8-070e-4b89-ad41-9324354f665d",
   "metadata": {},
   "source": [
    "8) LLM-Assisted Bias Explainability Tool\n",
    "\n",
    "- Goal: Use LLMs (with careful prompt engineering) to produce an explainable summary of potential bias: highlight omitted facts, framing choices, loaded words, and source gaps — then verify suggestions via rule-based checks.\n",
    "- Libraries: (local LLMs or API) transformers, prompt-tooling libs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3def79d-6991-467a-b527-308f922bf79a",
   "metadata": {},
   "source": [
    "9) Example\n",
    "> Input: article URL.  \n",
    "Actions: scrape text → compute sentiment + hedging score + entity frequency → compare entity sentiment to national average (across 10 outlets) → output: simple bias score + highlighted biased sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03275df-8b8d-4a68-a167-6ae498969308",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
